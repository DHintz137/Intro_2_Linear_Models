---
engine: knitr
webr: 
  show-startup-message: false  
  packages: ['ggplot2', 'patchwork', 'mcprofile']
format:
  revealjs:
    theme: [default, theme.scss]
    scrollable: true
    css: styles.css
    transition: slide
    highlight-style: github
    slide-number: c/t
    chalkboard: true
filters: 
 - webr
---


# [An Introduction to <br>Linear Models]{
  style="color:#001F3F; 
         font-size: 90px;
         text-shadow: -0.5px -0.5px 0 #000, 
                      0.5px -0.5px 0 #000, 
                      -0.5px 0.5px 0 #000, 
                      0.5px 0.5px 0 #000;"
}{background-image="images/max-harlynking-a9-P29h2V90-unsplash_adj2.png" background-size="cover" background-color="#4f6952"}


<h2 style="color:#1F67A4; font-size: 35px;">
    Daniel Hintz | 2024-03-25
</h2>

<h3 style="color:#4F483E; font-size: 25px;">
    A R Ladies Workshop
</h3>

<!--
343942
-->

```{r setup, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE, message = FALSE, purl = FALSE)
options(digits = 5)
```

# [Outline]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: incremental
::: {style="font-size: 1.2em;"}
-   Simple Linear Regression
-   Multiple Linear regression
-   Generalised Linear Models
-   Logistic Regression 
-   Linear Mixed Models
-   Generalised Linear Mixed Models
:::
:::



# [Preliminaries]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [Webr Code Execution]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

<p align="center">
  <img src="images/webr.gif" alt="webr" width="600px" style="border: 2px solid #000;"/>
</p>

::: incremental
- Use `cmd` `+` `enter` to execute a line.  

- As of `webR 0.2.3`, `webr` does not support smart execution, so for multiline code highlight the entire section before hitting `cmd` `+` `enter`.
:::

## [Scalars, Vectors and Matrices]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: incremental
- **Scalar:** A scalar is a single value.
$$
x = 5
$$

- **Vector:** A vector is a one-dimensional array of values.
$$
\underline{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
$$

- **Matrix:** A matrix is a two-dimensional array of values.
$$
A = \begin{bmatrix} 
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 
\end{bmatrix}
$$
:::

## [Variable Notation]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 


$$
\begin{array}{|c|c|c|}
\hline
 & \text{Roman (known)} & \text{Greek (unknown)} \\
 & \text{(Scalar, Vector, Matrix)} & \text{(Scalar, Vector, Matrix)} \\
\hline
\text{Random} & \mathbb{X}, \underline{X}, \mathbb{X} & \Theta, \underline{\Theta}, \boldsymbol{\Theta} \\
\text{Fixed} & x, \underline{x}, X & \theta, \underline{\theta}, \boldsymbol{\theta} \\
\hline
\end{array}
$$


## [Variable Notation (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: panel-tabset
### Fx,Rm

i.e., data (we observe it). 

- Scalar: $x = 7$
- Vector: $\underline{x}= \begin{bmatrix} 4 \\ 5 \end{bmatrix}$
- Matrix: $X = \begin{bmatrix} 4 & 6 \\ 8 & 10 \end{bmatrix}$


### Fx,Gr


i.e., Population Parameters, $\mu, \sigma,$ etc. 

- Scalar: $\theta$
- Vector: $\underline{\theta} = \begin{bmatrix} \theta_1 \\ \theta_2 \end{bmatrix}$ 
- Matrix: $\boldsymbol{\theta} = \begin{bmatrix} \theta_{11} & \theta_{12} \\ \theta_{21} & \theta_{22} \end{bmatrix}$


### Rd,Rm


Not seen in this Workshop. 


### Rd,Gr

We will not go into the details for Bayesian Statistics in this workshop.

- Scalar: $\Theta \sim dist()$
- Vector: $\underline{\Theta} \sim dist_i()$
- Matrix: $\boldsymbol{\Theta} \sim dist_{ij}()$


:::

## [Variable Notation (2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

In summary:

- **Random** variables represent values that are drawn from some probability distribution.
- **Fixed** variables represent known values or constants.
- **Roman letters** represent known quantities, while **Greek letters** typically represent unknown parameters that are estimated.

# [Simple Linear Regression]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [Simple Linear Regression (SLR)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1}
**Simple Linear Regression** is a statistical method used to model the relationship between two continuous variables: one independent variable (predictor) and one dependent variable (response).
:::

::: {.fragment fragment-index=2}
The goal is to find the best-fitting straight line (the regression line) that describes how the dependent variable changes as the independent variable changes.
:::

::: {.fragment fragment-index=3}
The model is expressed as:

$$
\begin{gathered}
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i \\
Y_i \sim N\left(\mu_i, \sigma^2\right)
\end{gathered}
$$
:::

::: {.fragment fragment-index=4}
Where:
:::

::: {.fragment fragment-index=5}
- $Y_i$ is the dependent variable (response) for observation $i$,
- $X_i$ is the independent variable (predictor) for observation $i$,
- $\beta_0$ is the intercept (the predicted value of $Y$ when $X = 0$),
- $\beta_1$ is the slope (the change in $Y$ for a one-unit change in $X$),
- $\epsilon_i$ is the error term (the difference between the observed and predicted values of $Y$).
:::

<!--
The assumptions of simple linear regression include linearity, independence of errors, normally distributed errors, and constant variance of errors (homoscedasticity). The method is widely used to predict or explain a response variable based on a single predictor.
-->

## [SLR (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}


::: {.fragment fragment-index=1}
Our expression for the mean as can be expressed as... 

$$
\begin{aligned}
g\left(\mathbb{E}\left(y_i\right)\right) &= \beta_0 + \beta_1 X_{i1} \\
g\left(\mu\right) &= \beta_0 + \beta_1 X_{i1} \\
\mu &= \beta_0 + \beta_1 X_{i1} \\
\mu &= \eta 
\end{aligned}
$$
:::

::: {.fragment fragment-index=2}
- $\beta_0, \beta_1$ are fixed coefficients to be estimated.
- $\epsilon_i \overset{iid}{\sim} N\left(0, \sigma_{\varepsilon}^2\right)$ is random and we can estimate its variance (where $\epsilon_i$ is a model error for observation $i$).
- $X$ is the fixed effects design matrix.
:::

::: {.fragment fragment-index=3}
From statistic theory $\mathbb{E}$ is called the expectation operator, where the expectation of a random variable is the mean, $\mu$. $g()$ is called the link function. The link function relates the mean response to the linear predictor $\eta$. 
:::

## [SLR (2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: {.fragment fragment-index=1}
**Design Matrix:**

$$
X=\left[\begin{array}{cccc}
1 & 631.69 & 33.57 & 0.187 \\
1 & 753.10 & 48.94 & 0.0246 \\
\vdots & \vdots & \vdots & \vdots \\
1 & 435.02 & 37.43 & 0.226
\end{array}\right]
$$

:::

::: {.fragment fragment-index=2}
Where the column of 1’s, or $\underline{1}$ is added to make the linear algebra work. 
:::

::: {.fragment fragment-index=3}
**Response Vector and Vector of model Errors**

$$
\underline{y}=\left[\begin{array}{c}
5.069 \\
5.016 \\
\vdots \\
4.380
\end{array}\right] \quad \underline{\epsilon}=\left[\begin{array}{c}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_{36}
\end{array}\right]
$$
:::

## [SLR Code (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

```{webr-r}
#| label: 'm1'
m1 <- lm(Sepal.Length ~ Sepal.Width, data = iris)
summary(m1)
```

## [SLR Code (2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}




```{webr-r}
#| label: 'p1'
#| message: false
p1 <- ggplot(iris, aes(x = Sepal.Width, y = Sepal.Length)) +
  geom_point() +  # Scatter plot of the data points
  geom_smooth(method = "lm", col = "#1F67A4") +  # Regression line
  labs(title = "Simple Linear Regression",
       x = "Sepal Width",
       y = "Sepal Length")
p1
```

## [Q1]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: panel-tabset

### Question

Fit a simple linear model to predict the weight (wt) of a car based on its horsepower (hp) from the mtcars dataset. What is the slope of the model?

```{webr-r}
# You can work here 
```

### Answer

```{webr-r}
# Fit the simple linear model
model1 <- lm(wt ~ hp, data = mtcars)

# Extract the slope (coefficient of hp)
coef(model1)[2]
```

:::

## [Q2]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: panel-tabset

### Question

Using the same model from Q1, predict the weight of a car with 150 horsepower. 

```{webr-r}
# You can work here 
```

### Answer

```{webr-r}
# Predict the weight of a car with 150 horsepower
predict(model1, data.frame(hp = 150))
```

:::

# [Multiple Linear Regression]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [Multiple Linear Regression (MLR)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}

::: {.fragment fragment-index=1}
$$
\begin{gathered}
\begin{aligned}
Y_i & =\beta_0+\beta_1 X_{i 1}+\ldots+\beta_k X_{i k}+\varepsilon_i \\
& =X \beta + \epsilon_i, ~i=1, \ldots, n
\end{aligned} \\
Y_i \sim N\left(\mu_i, \sigma^2\right)
\end{gathered}
$$


:::

::: {.fragment fragment-index=2}
- $\underline{\beta}=\left(\beta_0, \beta_1, \dots \beta_{k}\right)$ are fixed coefficients to be estimated 
Note: the estimate would be $\hat{\beta} = \text{some number}$, for example $\hat{\beta_0} = 2$.
- $\varepsilon_i \overset{iid}{\sim} N\left(0, \sigma_{\varepsilon}^2\right)$ is random and we can estimate its variance.
- $X$ is the fixed effects design matrix.
:::

## [MLR Code]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}


```{webr-r}
#| label: 'm2'
#| message: false
m2 <- lm(Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)
p2 <- ggplot(iris, aes(x = Sepal.Width, y = Sepal.Length)) +
  geom_point() +  
  geom_smooth(method = "lm", aes(x = Sepal.Width, y = predict(m2, iris)), color = "#0097A7") +
  labs(title = "Sepal Length vs Sepal Width",x = "Sepal Width",y = "Sepal Length")
p3 <- ggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +
  geom_point() + 
  geom_smooth(method = "lm", aes(x = Petal.Length, y = predict(m2, iris)), color = "#795796") +
  labs(title = "Sepal Length vs Petal Length",x = "Petal Length", y = "Sepal Length")
p2 + p3 # Combine both plots
```

## [Fixed vs Random?]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 


- $X_{i j}$ are fixed and known.
- $\beta_i$ are fixed and unknown; fixed effects.
- $\varepsilon_i \overset{iid}{\sim} N\left(0, \sigma_{\varepsilon}^2\right)$ ; $\varepsilon_i$ are random and unknown; a random effect.
- $\sigma_{\varepsilon}^2$ (fixed and unknown) is the variance component of the random effect.


## [Model Assumptions (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1}
Our Model assumptions, are mostly packed assumptions of the model errors. That is what we are assuming about the unknown population parameter $\epsilon_i$, where $\epsilon_i$ is the model error associated with $i^{th}$ observation for our modeled response $Y_i$. 

$$
\varepsilon_i \overset{iid}{\sim} N\left(0, \sigma_{\varepsilon}^2\right)
$$
:::

::: {.fragment fragment-index=2}
**a) Independently and Identically distributed (iid)** 


$$
\epsilon_i \color{#6A851D}{\overset{iid}{\sim}} N\left(0, \sigma_{\varepsilon}^2\right)
$$

**Independently Distributed:**
The experimental/observational units are independent of one another (ex. response per Sites/locations).

**Identically Distributed:**
The natural variability in the response (i.e. species richness) is the same regardless of the response's magnitude.
:::

## [Model Assumptions (2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=3}

**b)  Distributed Gaussian/Normal** 
$$
\varepsilon_i \overset{iid}{\sim} \color{#6A851D}{N}\left(0, \sigma_{\varepsilon}^2\right)
$$
:::



::: {.fragment fragment-index=4}
**c)  Mean Zero Model Errors**
$$
\varepsilon_i \overset{iid}{\sim} N\left(\color{#6A851D}{0}, \sigma_{\varepsilon}^2\right)
$$
:::

::: {.fragment fragment-index=5}
**d) Constant Model Error Variance** 

$$
\epsilon_i \overset{iid}{\sim} N\left(0, \color{#6A851D}{\sigma_{\varepsilon}^2}\right)
$$
There is **one** theoretical value for the variance across all $i$ observations.  
:::

::: {.fragment fragment-index=6}
**e) Mean is a Linear Function of the Predictors** 

$$
\begin{aligned}
g\left(\mathbb{E}\left(y_i\right)\right) &= \beta_0 + \beta_1 X_{i1} \\
g\left(\mu\right) &= \beta_0 + \beta_1 X_{i1} \\
\mu &= \beta_0 + \beta_1 X_{i1} 
\end{aligned}
$$
:::

<!--
- The mean in species richness is a linear function of the predictor variables (i.e. discharge, area, and proportion of discharge in the dries months)
 for example with logistic regression, supposing $\mu_i=E\left[y_i\right]=n_i \frac{1}{1+\exp \left(\beta_0 + \beta_1 X_{i1}\right)}$ ,  the mean is **NOT** a linear function of the predictors.

- We will gradually address violations of each of the above assumptions...let's first look at a couple of examples violating assumption 1 above.

-->


# [Generalised Linear Model]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [GLM (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 


::: {.incremental}
- Independent observations $Y_1, Y_2, \ldots, Y_n$.
- Each observation $Y_i$ has a distribution that is a member of the exponential family i.e., Normal (Gaussian), Bernoulli, Gamma, Beta, Poisson, chi-squared, etc. See full list [here](https://en.wikipedia.org/wiki/Exponential_family).
- $\operatorname{Var}\left(y_i\right)$ is a function of $\mu_i$.
- The model is derived from a link function, $\eta_i=g\left(\mu_i\right)=\mathbf{x}_i^{\prime} \boldsymbol{\beta}$, where $E\left(y_i\right)=g^{-1}\left(\eta_i\right)=g^{-1}\left(\mathbf{x}_i \boldsymbol{\underline { \beta }}\right)$.
:::

## [GLM (2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1}
When transitioning from the **Simple Linear Model (SLM)** to the **Generalized Linear Model (GLM)**, several key assumptions of the SLM are relaxed or modified to accommodate a wider range of data types and distributions.
:::


::: {.fragment fragment-index=2 style="font-size: 0.9em;"}
| **Assumption**                     | **Simple Linear Model (SLM)**                            | **Generalized Linear Model (GLM)**                                         |
| ---------------------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------- |
| **Distribution of Errors**         | Errors follow a normal distribution.                     | Response follows a distribution from the exponential family.               |
| **Constant Error Variance**        | Error variance is constant (homoscedasticity).           | Variance is allowed to vary as a function of the mean.                     |
| **Mean Function**                  | Mean of the response is a linear function of predictors. | Mean is linked to a linear predictor through a link function.              |
| **Model Errors**                   | Errors have a mean of zero.                              | Explicit errors are not modeled in the same way.                           |
| **Identically Distributed Errors** | Errors are identically distributed.                      | Responses are not identically distributed if variance depends on the mean. |
| **Independence of Observations**   | Observations are independent.                            | Observations are independent (this assumption is retained).                |
|                                    |                                                          |                                                                            |
:::


## [GLM (3)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 


The most common uses of GLM's include logistic(binomial) regression for proportion/binary data, Poisson Regression for count data, and Gamma regression for skewed data.

- We will look at logistic regression in this workshop.


# [Logisitc Regression]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [Logisitc Regression (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1}
- Consider a study analyzing survival rates from the Titanic disaster, where passengers are classified based on available demographic and ticket-related factors.
:::

::: {.fragment fragment-index=2}
- The primary outcome of interest is whether the passenger survived the disaster. 
:::

::: {.fragment fragment-index=3}
- For this case study, there were 891 total passengers, and the available information for each passenger includes the passenger's class, gender, age, number of siblings/spouses aboard, number of parents/children aboard, ticket number, fare, cabin number (if available), and port of embarkation.
:::

::: {.fragment fragment-index=4}
In this context, logistic regression could be applied to model the probability of survival (the dependent variable) based on various independent variables such as passenger class, gender, age, and fare.
:::

## [Logit Link Function]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 




::: {.fragment fragment-index=1}
**The Logit link function** links the probability to the linear predictor:

$$
\begin{gathered}
g\left(\mathbb{E}\left(Y_i\right)\right) = \text{logit}(\pi(\underline{x}_i)) 
= \log\left[\frac{\pi(\underline{x}_i)}{1 - \pi(\underline{x}_i)}\right] \\ 
\operatorname{logit}\left(\pi_i\right)=\underline{x}_i \boldsymbol{\beta}
\\
\log\left[\frac{\pi(\underline{x}_i)}{1 - \pi(\underline{x}_i)}\right] =\underline{x}_i \boldsymbol{\beta} \\
Y_i \sim \text{Bin}(1, \pi_i)
\end{gathered}
$$
:::

::: {.fragment fragment-index=2}
This is the logit link function, where:

- $\pi(\underline{x}_i)$ is the probability of success (i.e., $P(y_i = 1)$).
- $\underline{x}_i \boldsymbol{\beta}$ is the linear predictor, which is a linear combination of the predictors and their coefficients.

:::


::: aside
If were were modeling survival, say as groups of Shipwrecks Titanic, SS Sultana, etc we would have $Y_i \sim \text{Bin}(n_i, \pi_i)$.
:::

## [Inverse Logit Function]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1}
**Inverse logit function** (linking the linear predictor to the expected probability):
$$
\mu_i = \mathbb{E}[Y_i] = \pi(\underline{x}_i) = \frac{1}{1 + \exp\left(-\underline{x}_i^\top \boldsymbol{\beta}\right)}
$$

:::


::: {.fragment fragment-index=2}

- $Y_i$ denotes the number of survivors for the $i^{th}$ subject.
- $\pi_i$ denotes the probability of survival at the $i^{th}$ subject.
- $\underline{\beta}$ the vector of coefficients.  
- $\underline{x}_i$ refers to the vector of covariates (or features, predictors) for the $i^{\text{th}}$ observation.
:::


## [Logisitc Regression Code]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

- The estimated coefficients are given in the log odds scale. If we want odds ratios, we need to exponentiate the CI.

```{webr-r}
#| label: 'titanic logistic regression'
titanic <- read.csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")
m3 <- glm(Survived~Age, family="binomial",data=titanic)
K <- matrix(c(0, 1), ncol = 2)
linComb <- mcprofile::mcprofile(m3, K)
logOddsCI <- confint(linComb, level = 0.95)
exp(logOddsCI)
```

For a 1 year increase in passengers age, the estimated estimated odds of a patient surviving decreases by 1.1 %. I am 95% confident that the interval (0.979, 0.999), captures the true ratio in the odds of a patient having survived the sinking of the titanic.

# [Linear Mixed Model]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [Linear Mixed Model (1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1}
A **Linear Mixed Model (LMM)** is an extension of the standard linear regression model that allows for both **fixed effects** and **random effects** in the analysis. It is commonly used when data are grouped or clustered, and it accounts for both **within-group** and **between-group** variability.
:::

::: {.fragment fragment-index=2}
- **Fixed Effects**: These are the traditional regression coefficients that represent the overall population-level relationships between predictors and the outcome. In a fixed-effects model, these effects are assumed to be the same across all individuals or groups.
:::

::: {.fragment fragment-index=3}
- **Random Effects**: These represent group-specific variations. Random effects are used when you expect certain groups (e.g., subjects, locations, times) to have their own individual deviations from the overall trend. These effects allow each group to have its own intercept or slope, drawn from a distribution.
:::

::: {.fragment fragment-index=4}
**Applications of LMM:**
:::

::: {.fragment fragment-index=5}
- **Longitudinal data**: Repeated measurements from the same subjects over time (e.g., medical studies tracking patients' health).
:::

::: {.fragment fragment-index=6}
- **Multilevel data**: Data that is structured hierarchically (e.g., students nested within classrooms, patients nested within hospitals).
:::

## [LMM; Wood Pigeon  Example]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

:::{.incremental}
- Westbrooke and Robinson (2009) reported on monitoring efforts of the **Kukupa (or Wood Pigeon)**, New Zealand’s only native pigeon, in a study area exposed to intensive pest control. 

- Stations in the Bream Head area were selected at random and abundance was recorded at each of 10 stations for a period of 12 years; **we will index i for site and j for year**. 

- In addition to abundance, a measure of noise level was also recorded at each time point for each station. Data are provided in “Kukupa.csv”. 

- Most Stations start with very low abundances and then tend to increase over time.
:::

<!--
...Stations 82 vs. 90
-->

```{r Read-in-Kukupa}
#| echo: false
Kukupa <- read.csv("Kukupa.csv")
Kukupa$sqrt_abund <- sqrt(Kukupa$abundance)
Kukupa$FStation <- as.factor(Kukupa$Station)
```

## [Primary Questions]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

:::{.incremental}
- Have wood pigeons (Kukupa) increased in abundance over time?
- Since counts were observed at several stations in this study area, if there has been an increase in average abundance, how consistent is this effect across all stations?
:::


## [Kukupa Data Structure]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {style="margin-top: 40px;"}
:::

<p align="center">
  <img src="images/Kukupa_data_structure.png" alt="Kukupa data structure" width="800px" style="border: 2px solid #000;"/>
</p>

:::{.incremental}
- Differences among Stations could be due to variation in unmeasured factors such as: forest composition, elevation, pest densities, etc.
- We can explicitly model Station to Station differences with a random effect.
- We also have within Station variation in abundance (ex. measurement error, time of day, etc).
:::

## [SLR for Kukupa Monitoring]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 


$$
y_{i j}=\beta_0+\beta_1 t_{i j}+\varepsilon_{i j} \quad(i=1,2, \ldots, 10 ; \mathrm{j}=1,2, \ldots, 12)
$$

:::{.incremental}
- The model above does not allow for Station to Station variation to be modeled explicitly.
- Since the intercept reflects initial abundances (i.e. abundances in $t=0$ ), we can think of a random intercept model to account for Station to Station variation.
:::



## [Random Intercept Mixed Model]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1 style="font-size: 0.8em;"}
$$
\begin{gathered}
y_{i j}=\beta_0+\beta_1 t_{i j}+\delta_{0 i}+\varepsilon_{i j} \quad(i=1,2, \ldots, 10 ; \mathrm{j}=1,2, \ldots, 12) \\ 
\delta_i  \overset{\text{iid}}{\sim} N\left(0, \sigma_\delta^2\right) \quad \varepsilon_{i j} \overset{\text{iid}}{\sim} N\left(0, \sigma_{\varepsilon}^2\right) \quad Y \sim N\left(X \underline{\beta}, Z G Z^{\prime}+R\right)
\end{gathered}
$$

:::

::: {.fragment fragment-index=2}
<p align="center">
  <img src="images/Kukupa_RI.png" alt="Kukupa Random intercepts" width="500px" style="border: 2px solid #000;"/>
</p>
:::

::: {.fragment fragment-index=3 style="font-size: 0.8em;"}
- The random intercept model **reflects variation in the initial abundances** (i.e. abundances in $t=0$ ):
:::

::: {.fragment fragment-index=4 style="font-size: 0.8em;"}

$$
Response Model: \\
\begin{aligned}
& y_{1,1}=\beta_0+\beta_1 t_{1,1}+\delta_{01}+\varepsilon_{1,1} \\
& \vdots \\
& y_{10,12}=\beta_0+\beta_1 t_{10,12}+\delta_{0,10}+\varepsilon_{10,12}
\end{aligned}
$$
:::

::: aside
Note: $Z$ is the incidence matrix, $G$ is the matrix $Var(\delta)$, and $R$ is the matrix $Var(\epsilon)$.
:::

## [LMM Code]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

```{r R_Int_mod}
library(lme4)
R_Int_mod <- lmer(sqrt_abund ~ Scale_Yr + (1|FStation),
data=Kukupa)
```

```{r}
#| code-fold: true
#| collapse: true
#| code-summary: "Model Summary"
summary(R_Int_mod)
```

```{r}
#| code-fold: true
#| collapse: true
#| code-summary: "Random Effects"
ranef(R_Int_mod)
```

**Illustrating Predictions**

$$
\begin{aligned}
&\begin{aligned}
\sqrt{\hat{\mu}}= & \hat{\beta}_o+\hat{\beta}_y \text { year } \\
& =0.15356+0.05245 t_{1 j}
\end{aligned}\\
&\begin{aligned}
\sqrt{\hat{\mu}_{90}} & =\hat{\beta}_o+\hat{\beta}_1 \text { year }+\hat{\delta}_{0,90} \\
& =(0.15356+0.05479)+(0.05245) t_{1 j} \\
& =0.20835+0.05245 t_{1 j}
\end{aligned}
\end{aligned}
$$

## [Random Slopes Mixed Model]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 




::: {.fragment fragment-index=1 style="font-size: 0.8em;"}
$$
\begin{gathered}
y_{i j}=\beta_0+ (\beta_1 + \delta_{1i}) t_{i j} +\epsilon_{i j} \quad(i=1,2, \ldots, 10 ; \mathrm{j}=1,2, \ldots, 12)\\ 
\delta_i  \overset{\text{iid}}{\sim} N\left(0, \sigma_\delta^2\right) \quad \varepsilon_{i j} \overset{\text{iid}}{\sim} N\left(0, \sigma_{\varepsilon}^2\right) \quad Y \sim N\left(X \underline{\beta}, Z G Z^{\prime}+R\right)
\end{gathered}
$$
:::

::: {.fragment fragment-index=2}
<p align="center">
  <img src="images/Kukupa_RS.png" alt="Kukupa Random intercepts" width="700px" style="border: 2px solid #000;"/>
</p>
:::

::: {.fragment fragment-index=3 style="font-size: 0.8em;"}
- Here, instead of allowing the modeling of fluctuations in the initial abundances (the intercepts), we allow for the modeling of varying trends over time from Station to Station. 
:::

::: {.fragment fragment-index=4}
$$
\begin{aligned}
y_{i j} & =\beta_0 +\beta_1 t_{i j}+\delta_{1 i} t_{i j}+\varepsilon_{i j} \quad(i=1,2, \ldots, 10 ; \mathrm{j}=1,2, \ldots, 12) \\
& =\beta_0+\left(\beta_1+\delta_{1 i}\right) t_{i j}+\varepsilon_{i j}
\end{aligned}
$$

:::

## [Random Slopes LMM Code]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

<!--
```{r test}
#| class-source: smaller_output_class1
#| classes: smaller_output_class1
list("a", "b")
```
-->

```{r R_Slope_only_mod}
R_Slope_only_mod <- lmer(sqrt_abund ~ Scale_Yr + (0 + Scale_Yr|FStation), data=Kukupa)
```

```{r}
#| code-fold: true
#| collapse: true
#| code-summary: "Model Summary"
summary(R_Slope_only_mod)
```


```{r}
#| code-fold: true
#| collapse: true
#| code-summary: "Random Effects"
ranef(R_Slope_only_mod)
```

**Illustrating Predictions**

$$
\begin{aligned}
&\begin{aligned}
\sqrt{\hat{\mu}}= & \hat{\beta}_o+\hat{\beta}_y \text { year } \\
& =0.15356+0.05245 t_{1 j}
\end{aligned}\\
&\begin{aligned}
\sqrt{\hat{\mu}_{90}} & =\hat{\beta}_o+\hat{\beta}_1 t_{1 j}  + \hat{\delta}_{1,90} t_{1 j}\\
& =(0.15356)+(0.05245 + 0.02166014) t_{1 j} \\
& =0.15356 + 0.07411 t_{1 j}
\end{aligned}
\end{aligned}
$$


## [Random Intercept and Random Slope Mixed Model]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

Or if we want we can allow for a random intercept and a random slope. 

$$
\begin{aligned} y_{i j} & =\beta_0+\delta_{0 i}+\beta_1 t_{i j}+\delta_{1 i} t_{i j}+\varepsilon_{i j} \quad(i=1,2, \ldots, 10 ; \mathrm{j}=1,2, \ldots, 12) \\ & =\left(\beta_0+\delta_{0 i}\right)+\left(\beta_1+\delta_{1 i}\right) t_{i j}+\varepsilon_{i j}\end{aligned}
$$



<p align="center">
  <img src="images/Kukupa_RI_RS.png" alt="Kukupa Random intercepts" width="700px" style="border: 2px solid #000;"/>
</p>


## [Random Intercepts and Slopes Code]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

```{r}
R_Int_and_Slope_mod <- lmer(sqrt_abund ~ Scale_Yr + (Scale_Yr|FStation), data=Kukupa)
```

```{r}
#| code-fold: true
#| collapse: true
#| code-summary: "Model Summary"
summary(R_Int_and_Slope_mod)
```


```{r}
#| code-fold: true
#| collapse: true
#| code-summary: "Random Effects"
ranef(R_Int_and_Slope_mod)
```

**Illustrating Predictions**

$$
\begin{aligned}
&\begin{aligned}
\sqrt{\hat{\mu}}= & \hat{\beta}_0+\hat{\beta}_1 t_{1 j}  \\
& =0.15356+0.05245 t_{1 j}
\end{aligned}\\
&\begin{aligned}
\sqrt{\hat{\mu}_{90}} & =\hat{\beta}_o+\hat{\beta}_I t_{1 j} +\hat{\delta}_{0,90}+\hat{\delta}_{1,90} t_{1 j} \\
& =(0.15356-0.12419)+(0.05245+0.0385) t_{1 j} \\
& =0.02937 + 0.09095 t_{1 j}
\end{aligned}
\end{aligned}
$$


# [Generalized Linear Mixed Model]{style="color:#001F3F; font-size: 70px;"}{background-image="images/background_2.png" background-size="cover"}

## [GLMM(1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

- Generalized linear mixed models (GLMMs) are an extension of linear mixed models to allow response variables from different distributions than the Normal
- Binary data
- Count data
- Survival Data
- Other members of the exponential family
- In addition, rather than modeling the responses directly, some link function is applied to the mean in order to guarantee 'legal' estimates of the mean

## [Linear Predictor and Link Function]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

- Let the linear predictor, $\boldsymbol{\eta}$, be the combination of the fixed and random effects, excluding the residuals

$$
\boldsymbol{\eta}=\mathbf{X} \boldsymbol{\beta}+\mathbf{Z} \boldsymbol{\delta}
$$

$$
\begin{gathered}
g\left(~.\right)=\text { link function } \\
h\left(~.\right)=g^{-1}\left(~.\right)=\text { inverse link function } \\
\mathbf{g}(E(\mathbf{y}))=\boldsymbol{\eta}
\end{gathered}
$$

## [Forbs Example(1)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}  

::: {.fragment fragment-index=1 style="font-size: 0.8em;"}
- Below are the pictures of the Rocky Flats National Wildlife Refuge before the final clean‐up of the industrial facility that was once located there (lower left) and what this site looked like after the clean‐up in 2014 (it looks similar to this now). 
:::

::: {.fragment fragment-index=2 style="font-size: 0.8em;"}
<p align="center">
  <img src="images/forbs.png" alt="forbs" width="500px" style="border: 2px solid #000;"/>
</p>
:::

## [Forbs Example(2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1 style="font-size: 0.8em;"}
- One of the management questions at the refuge concerns the relative proportion of forbs on the refuge and whether this relative proportion has changed substantially since the time period in which the site was intensely monitored (1996‐2000) and a recent monitoring effort in 2022. 
:::

::: {.fragment fragment-index=2 style="font-size: 0.8em;"}
- Vegetation cover is measured by laying out 2x50 meter transects and then walking the transect in two directions, stopping every 1m to shoot a laser to the ground and noting the ground cover at the laser point (bare ground, biotic material, species of plant present if a plant is located at the laser point, etc). 
:::

::: {.fragment fragment-index=3 style="font-size: 0.8em;"}
- The file forbs.csv provides data on forb abundance observed during the observation years for each transect. The field ‘sumcov’ denotes the total number of times that the laser pointed to native forb species and ‘totcov’ denotes the total number of laser points which landed on biotic material. 
:::

::: {.fragment fragment-index=4 style="font-size: 0.8em;"}
- If you take the ratio of sumcov to totcov, you will have the relative abundance of forbs within the vegetative areas of the transect. Assume that totcov is fixed and that sumcov conditioned on a transect/year combination follows a binomial distribution. 
:::

::: {.fragment fragment-index=5 style="font-size: 0.8em;"}
- A spatially balanced sampling design was utilized to select the transects for the study and the same transects were walked in each of the years of the study.
:::

::: {.fragment fragment-index=6 style="font-size: 0.8em;"}
- we will first treat 'YEAR' as a categorical variable and assume a logit link for relating the relative abundance of forbs to 'YEAR' as well as 'Transect'. We will also scale year such that 1996 is the baseline year. 
:::

## [GLMM(2)]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}  

::: {.fragment fragment-index=1 style="font-size: 0.8em;"}
$$
\begin{gathered}
Y_i \sim \operatorname{Bin}\left(n_i, \pi_i\right) \\
\operatorname{logit}\left(\pi_i\right)=\mathbf{X}_i \underline{\beta}+\mathbf{Z}_i \delta_i
\end{gathered}
$$


$$
\underline{\beta} = 
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3 \\
\beta_4 \\
\beta_5 \\
\end{bmatrix}
$$
:::

::: {.fragment fragment-index=2 style="font-size: 0.7em;"}
- $\underline{\beta}$ is the vector of fixed effects ($\beta_0$ and $\beta_1$ through to $\beta_5$)
- $\beta_0$ is the population relative abundance of forbs at year 0 over all transects
- $\beta_1$ through to $\beta_5$ is the change in the log odds from the reference category (year 0) for every other year (year 1 through year 5)  for the relative abundance of forbs. 
- $\boldsymbol{\delta}$ is the vector of random effects (the mean relative abundance of forbs for each transect). 
- $\mathbf{X}$ is the fixed effects design matrix which comes from our predictors
- $\mathbf{Z}$ is the random effects incidence matrix.
:::

## [Forbs GLMM Code]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"} 

::: {.fragment fragment-index=1 style="font-size: 0.7em;"}
Interpreting the coefficients from GLMM with a proportion as the response and assuming a binomial distribution makes the coefficients difficult. So we will focus on how to fit the models in code, perform model selection, and plot the bootstrapped conditional predictions.
:::

::: {.fragment fragment-index=2}

```{r}
library(lme4)
forbs <- read.csv("forbs.csv")
forbs$sc_year <- as.factor(forbs$sc_year)
levels(forbs$sc_year) <- as.character(0:5)
R.abnd.forb <- with(forbs,cbind(sumcov, totcov - sumcov))
forbs$R.abnd.forb <- R.abnd.forb

# ci.cs (logistic regression)
m0 <- glm(R.abnd.forb ~ sc_year, family = binomial(link = "logit"),
           data=forbs)
# ri.cs
m1 <- glmer(R.abnd.forb ~ sc_year + (1|TRANSECT), family = binomial(link = "logit"),
           data=forbs)
# ri.rs(ind)
m2 <- glmer(R.abnd.forb ~ sc_year + (1|TRANSECT)+(0+sc_year|TRANSECT), 
            family = binomial(link = "logit"),data=forbs)
# ci.rs
m3 <- glmer(R.abnd.forb ~ sc_year + (0+sc_year|TRANSECT), 
            family = binomial(link = "logit"),data=forbs)
# ri.rs(cor)
m4 <- glmer(R.abnd.forb ~ sc_year + (1+sc_year|TRANSECT), 
            family = binomial(link = "logit"),data=forbs)
```
:::

## [Model Selection]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}  

::: {.fragment fragment-index=1}
```{r}
mods <- list(m0,m1,m2,m3,m4)
mods.AIC <- unlist(lapply(mods, AIC))
names(mods.AIC) = c("ci.cs", "ri.cs", "ri.rs(ind)", "ci.rs", "ri.rs(cor)")
(mods.AIC <- sort(mods.AIC))
```
:::

::: {.fragment fragment-index=2}
**Chi squared test for comparing correlated and Uncorrelated Slopes** 

```{r}
anova(m2,m4)
```

:::

::: {.fragment fragment-index=3 style="font-size: 0.7em;"}
$$
\begin{aligned}
H_0 &: \text{The full Model is not different than the reduced model} \\
H_A &: \text{The reduced Model is different than the reduced model}
\end{aligned}
$$
:::

::: {.fragment fragment-index=4 style="font-size: 0.7em;"}
From AIC we see that correlated random intercepts and random slopes is the preferred model as well as being supported by our test between `m2` (independent random intercepts and random slopes) and `m4` (correlated random intercepts and random slopes) with a p-value small than our $\alpha$ of 0.05.
:::

## [Bootstrapped Predictions]{style="color:#001F3F; font-size: 55px;"}{background-image="images/background_1.png" background-size="cover"}  


Now, considering ‘YEAR’s as categorical, we will find the 95% boot‐strapped confidence intervals for the relative proportion of forbs in each year on the refuge.

```{r boostrap, cache=TRUE}
library(tictoc)
library(ggplot2)

toPropMarg <- function(mod){
  X <- model.matrix(mod); X <- X[,1:ncol(X)]
  beta <- as.matrix(fixef(mod))
(1/(1+exp(-X  %*% beta)))[1:6] # marginals 
}

forbs$sc_year <- as.factor(forbs$sc_year)
levels(forbs$sc_year) <- as.character(0:5)

# ri.cs
m1 <- glmer(R.abnd.forb ~ sc_year + (1|TRANSECT), family = binomial(link = "logit"),
           data=forbs)

tic() # timing run time 
bb <- bootMer(m1, FUN=toPropMarg, nsim = 200,
              ncpus = parallel::detectCores() - 1, parallel = "multicore")
toc()

lci <- apply(bb$t, 2, quantile, 0.025)   
uci <- apply(bb$t, 2, quantile, 0.975)   
med <- apply(bb$t, 2, quantile, 0.5)   
sdev <- apply(bb$t, 2, sd)   

ci_df <- data.frame(
  Year = as.character(unique(forbs$YEAR)),
  est = med, sd = sdev, Lower = lci,Upper = uci)

ggplot(ci_df, aes(x = Year, ymin = Lower, ymax = Upper)) +
  geom_errorbar(width = 0.2) +  # Adjust the width as needed
  theme_minimal() +
  labs(title = "95% Conditional Bootstrapped Confidence Intervals for Each Year",
       x = "Year",
       y = "Proportion of Forbs")
```


# Thank you!! {.center background-color="#447099"}


`r fontawesome::fa("linkedin", "black")` <a href="https://www.linkedin.com/in/daniel-hintz-rstat/" class="custom-link">daniel-hintz</a>

`r fontawesome::fa("github", "black")` <a href="https://github.com/DHintz137" class="custom-link">DHintz137</a>

`r fontawesome::fa("globe", "black")` <a href="https://dhintz137.github.io/quartz/" class="custom-link">dhintz137.github.io</a>

`r fontawesome::fa("envelope", "black")` <a href="dhintz1@uwyo.edu" class="custom-link">dhintz1@uwyo.edu</a>